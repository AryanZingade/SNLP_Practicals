{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4SMTwwN1An4L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/IMDB Dataset 2.csv\")"
      ],
      "metadata": {
        "id": "X1P4jVw8Aoas"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7q8ewNGAxAX",
        "outputId": "5a19e03e-4164-440b-e8bf-bde8639515e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
      ],
      "metadata": {
        "id": "CeI7cWI3BLAB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "86XeHRFPBXi5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_counts = vectorizer.transform(test_df['cleaned_text'])\n",
        "\n",
        "tokenized_corpus = [word_tokenize(text) for text in train_df['cleaned_text']]\n",
        "w2v_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3RXHT1YBZCi",
        "outputId": "8426b735-8d05-4a81-fae6-b51fa80b562b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46378463, 48637980)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_w2v(text, model, vector_size):\n",
        "    words = word_tokenize(text)\n",
        "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(word_vecs) == 0:\n",
        "        return torch.zeros(vector_size)\n",
        "    return torch.tensor(sum(word_vecs) / len(word_vecs))\n",
        "\n",
        "train_w2v = torch.stack([text_to_w2v(text, w2v_model, 100) for text in train_df['cleaned_text']])\n",
        "test_w2v = torch.stack([text_to_w2v(text, w2v_model, 100) for text in test_df['cleaned_text']])"
      ],
      "metadata": {
        "id": "GXdNGgy-Bc17"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(train_w2v, torch.tensor(train_df['label'].values))\n",
        "test_dataset = TextDataset(test_w2v, torch.tensor(test_df['label'].values))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "K8GWTQGnCTJj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x.unsqueeze(1), h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "input_size = 100\n",
        "hidden_size = 50\n",
        "output_size = 2\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "NBItMiozClEs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'epoch : [{epoch+1}/{epochs}], loss : {running_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "id": "puN-5tI9Co2v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.float())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'accuracy of test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "ViTKZ6StCtpL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, criterion, optimizer, epochs=15)\n",
        "test_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJHqOPcOCzXU",
        "outputId": "1653c486-a724-490b-8b74-37cad31a3042"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : [1/15], loss : 0.3297\n",
            "epoch : [2/15], loss : 0.3150\n",
            "epoch : [3/15], loss : 0.3106\n",
            "epoch : [4/15], loss : 0.3076\n",
            "epoch : [5/15], loss : 0.3041\n",
            "epoch : [6/15], loss : 0.3023\n",
            "epoch : [7/15], loss : 0.2994\n",
            "epoch : [8/15], loss : 0.2973\n",
            "epoch : [9/15], loss : 0.2958\n",
            "epoch : [10/15], loss : 0.2946\n",
            "epoch : [11/15], loss : 0.2940\n",
            "epoch : [12/15], loss : 0.2921\n",
            "epoch : [13/15], loss : 0.2916\n",
            "epoch : [14/15], loss : 0.2905\n",
            "epoch : [15/15], loss : 0.2895\n",
            "accuracy of test set: 87.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = vector\n",
        "    return embeddings_index"
      ],
      "metadata": {
        "id": "qIsQPx-PC2FZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = \"/mnt/glove.6B.100d.txt\"\n",
        "embeddings_index = load_glove_embeddings(glove_file)"
      ],
      "metadata": {
        "id": "I53110JjGK-T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_glove(text, embeddings_index, vector_size):\n",
        "    words = word_tokenize(text)\n",
        "    word_vecs = [embeddings_index[word] for word in words if word in embeddings_index]\n",
        "    if len(word_vecs) == 0:\n",
        "        return torch.zeros(vector_size)\n",
        "    return torch.tensor(sum(word_vecs) / len(word_vecs))\n",
        "\n",
        "train_glove = torch.stack([text_to_glove(text, embeddings_index, 100) for text in train_df['cleaned_text']])\n",
        "test_glove = torch.stack([text_to_glove(text, embeddings_index, 100) for text in test_df['cleaned_text']])"
      ],
      "metadata": {
        "id": "VVe7nFgCGPmD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(train_glove, torch.tensor(train_df['label'].values))\n",
        "test_dataset = TextDataset(test_glove, torch.tensor(test_df['label'].values))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "7fn-GA6oGcnn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "t25Qtoi0GtmX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 100\n",
        "hidden_size = 50\n",
        "output_size = 2\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "d414w2zxGve5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "id": "XWId1Q9OGyty"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.float())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'accuracy on test set: {100 * correct / total:.2f}%')\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=10\n",
        "           )\n",
        "test_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DgLQLRYG0lM",
        "outputId": "0804ac21-2714-45aa-95d1-d9d409a46848"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5126\n",
            "Epoch [2/10], Loss: 0.4517\n",
            "Epoch [3/10], Loss: 0.4482\n",
            "Epoch [4/10], Loss: 0.4442\n",
            "Epoch [5/10], Loss: 0.4410\n",
            "Epoch [6/10], Loss: 0.4386\n",
            "Epoch [7/10], Loss: 0.4364\n",
            "Epoch [8/10], Loss: 0.4335\n",
            "Epoch [9/10], Loss: 0.4327\n",
            "Epoch [10/10], Loss: 0.4308\n",
            "accuracy on test set: 80.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc3bsFxqG6NQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}