{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pretrained word2vec-google-news-300 model\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'music':\n",
      "classical_music: 0.7197794318199158\n",
      "jazz: 0.683463990688324\n",
      "Music: 0.6595721244812012\n",
      "Without_Donny_Kirshner: 0.6416222453117371\n",
      "songs: 0.6396344900131226\n",
      "musicians: 0.6336299180984497\n",
      "tunes: 0.6330114603042603\n",
      "musical: 0.6186030507087708\n",
      "Logue_typed: 0.6150090098381042\n",
      "musics: 0.6148059368133545\n",
      "\n",
      "\n",
      "Top 10 words similar to 'movie':\n",
      "film: 0.8676770329475403\n",
      "movies: 0.8013108372688293\n",
      "films: 0.7363012433052063\n",
      "moive: 0.683036208152771\n",
      "Movie: 0.6693680882453918\n",
      "horror_flick: 0.6577848792076111\n",
      "sequel: 0.6577792763710022\n",
      "Guy_Ritchie_Revolver: 0.6509752869606018\n",
      "romantic_comedy: 0.6413198709487915\n",
      "flick: 0.6321909427642822\n",
      "\n",
      "\n",
      "Top 10 words similar to 'computer':\n",
      "computers: 0.7979379296302795\n",
      "laptop: 0.6640493273735046\n",
      "laptop_computer: 0.6548868417739868\n",
      "Computer: 0.647333562374115\n",
      "com_puter: 0.6082080006599426\n",
      "technician_Leonard_Luchko: 0.566274881362915\n",
      "mainframes_minicomputers: 0.5617720484733582\n",
      "laptop_computers: 0.5585449934005737\n",
      "PC: 0.5539618730545044\n",
      "maker_Dell_DELL.O: 0.5519253611564636\n",
      "\n",
      "\n",
      "Top 10 words similar to 'food':\n",
      "foods: 0.6804922819137573\n",
      "Food: 0.6538904905319214\n",
      "foodstuffs: 0.6425830125808716\n",
      "meals: 0.6166686415672302\n",
      "food_stuffs: 0.5928642153739929\n",
      "nourishing_meals: 0.5847609043121338\n",
      "foodstuff: 0.5835224390029907\n",
      "staple_foods: 0.5535789132118225\n",
      "nutritious: 0.5466451048851013\n",
      "meal: 0.5433712601661682\n",
      "\n",
      "\n",
      "Top 10 words similar to 'travel':\n",
      "traveling: 0.6823130249977112\n",
      "Travel: 0.6577276587486267\n",
      "travelers: 0.5849088430404663\n",
      "trips: 0.5770835280418396\n",
      "travels: 0.5704988241195679\n",
      "trip: 0.569098174571991\n",
      "journeys: 0.5535728931427002\n",
      "airfare: 0.5398489832878113\n",
      "Travelling: 0.5369202494621277\n",
      "Traveling: 0.5305293202400208\n",
      "\n",
      "\n",
      "Analogy: king - man + woman = queen (similarity: 0.7118192911148071)\n",
      "Analogy: paris - france + italy = lohan (similarity: 0.5069674253463745)\n",
      "Analogy: apple - fruit + vegetable = potato (similarity: 0.5865277051925659)\n"
     ]
    }
   ],
   "source": [
    "# Pick 5 words and find similar words\n",
    "words = [\"music\", \"movie\", \"computer\", \"food\", \"travel\"]\n",
    "\n",
    "for word in words:\n",
    "    similar_words = model.most_similar(word, topn=10)\n",
    "    print(f\"Top 10 words similar to '{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"{similar_word}: {similarity}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Test analogies\n",
    "analogies = [\n",
    "    (\"king\", \"man\", \"woman\"),\n",
    "    (\"paris\", \"france\", \"italy\"),\n",
    "    (\"apple\", \"fruit\", \"vegetable\")\n",
    "]\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    result = model.most_similar(positive=[word1, word3], negative=[word2])\n",
    "    print(f\"Analogy: {word1} - {word2} + {word3} = {result[0][0]} (similarity: {result[0][1]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aryan_zingade/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aryan_zingade/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aryan_zingade/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/opt/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  one reviewer mentioned watching oz episode you...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically there family little boy jake think t...  \n",
      "4  petter matteis love time money visually stunni...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove email addresses\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned_reviews'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned reviews\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [review.split() for review in df['cleaned_reviews']]\n",
    "cbow_model =Word2Vec(sentences, vector_size=100, window=5, workers=4, epochs=10, min_count=5, sg=0)\n",
    "cbow_model.save(\"cbow_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgram_model =Word2Vec(sentences, vector_size=100, window=5, workers=4, epochs=10, min_count=5, sg=1)\n",
    "sgram_model.save(\"sgram_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      4961\n",
      "           1       0.87      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n",
      "CBoW Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      4961\n",
      "           1       0.86      0.87      0.87      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_vector(rev, model, vector_size):\n",
    "    vector = np.zeros(vector_size)\n",
    "    count = 0\n",
    "    for word in rev.split():\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "    return vector\n",
    "\n",
    "# Generate feature vectors using different models\n",
    "vector_size = 100  # Update based on model's vector size\n",
    "df['skipgram_vector'] = df['cleaned_reviews'].apply(lambda x: get_vector(x, sgram_model, vector_size))\n",
    "df['cbow_vector'] = df['cleaned_reviews'].apply(lambda x: get_vector(x, cbow_model, vector_size))\n",
    "\n",
    "# Convert vectors to lists for training\n",
    "X_skipgram = np.vstack(df['skipgram_vector'].to_numpy())\n",
    "X_cbow = np.vstack(df['cbow_vector'].to_numpy())\n",
    "\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Split the data\n",
    "X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(X_skipgram, y, test_size=0.2, random_state=42)\n",
    "X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model for Skip-gram vectors\n",
    "model_sg = LogisticRegression()\n",
    "model_sg.fit(X_train_sg, y_train_sg)\n",
    "y_pred_sg = model_sg.predict(X_test_sg)\n",
    "print(\"Skip-gram Model Performance:\")\n",
    "print(classification_report(y_test_sg, y_pred_sg))\n",
    "\n",
    "# Train and evaluate the model for CBoW vectors\n",
    "model_cb = LogisticRegression()\n",
    "model_cb.fit(X_train_cb, y_train_cb)\n",
    "y_pred_cb = model_cb.predict(X_test_cb)\n",
    "print(\"CBoW Model Performance:\")\n",
    "print(classification_report(y_test_cb, y_pred_cb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
